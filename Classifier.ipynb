{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VarunVejalla/Answer-Extraction-ASSIP/blob/master/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1iz6fT7vl6",
        "colab_type": "text"
      },
      "source": [
        "# ASSIP BERT Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfeBLY-FCAO",
        "colab_type": "text"
      },
      "source": [
        "Import python resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4re6bbpEqE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install -e git+https://github.com/negedng/bert-embedding#egg=bert_embedding\n",
        "!pip install transformers\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn import model_selection\n",
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "import pickle # needed for opening entities\n",
        "import json # needed for squad data\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKEMfVfzFHRo",
        "colab_type": "text"
      },
      "source": [
        "Read in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo5eAFGVExF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in csv\n",
        "entitiesf1 = pd.read_csv(\"entities_train.csv\")\n",
        "testingf1 = pd.read_csv(\"entities_test.csv\")\n",
        "\n",
        "# loads in the squad datas\n",
        "with open(\"train-v2.0.json\") as file:\n",
        "  squad_train = json.load(file)\n",
        "\n",
        "with open(\"dev-v2.0.json\") as file:\n",
        "  squad_test = json.load(file)\n",
        "\n",
        "# to test with the coreferences resolved, we used this command\n",
        "# file = open(\"resolvedTexts\", \"rb\")\n",
        "# squad_train = pickle.load(file)\n",
        "\n",
        "# file = open(\"resolvedTexts_test\", \"rb\")\n",
        "# squad_test = pickle.load(file)\n",
        "\n",
        "# to cut time in the model, we created the embeddings beforehand for all passages, and then loaded those in.\n",
        "# After loading them in and saving them to a list called doc_embeddings and testing_embeddings, we deleted the original variable names\n",
        "# with garbage collector."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0ULyBRw-C3H",
        "colab_type": "text"
      },
      "source": [
        "Import BERT Embeddings and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NUDw0hNE9PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert_embedding import BertEmbedding\n",
        "from transformers import BertTokenizer\n",
        "bert_embedding = BertEmbedding(max_seq_length=512)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odRGuA--PBIy",
        "colab_type": "text"
      },
      "source": [
        "Convert data into series (for regular text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z_KfPfoI8Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_para_per_doc = list()\n",
        "for doc in range(len(squad_train[\"data\"])):\n",
        "  paragraphs = np.array([squad_train[\"data\"][doc][\"paragraphs\"][para][\"context\"] for para in range(len(squad_train[\"data\"][doc][\"paragraphs\"]))])\n",
        "  paragraph_series = pd.Series(paragraphs)\n",
        "  list_of_para_per_doc.append(paragraph_series)\n",
        "\n",
        "list_of_para_per_doc_test = list()\n",
        "for doc in range(len(squad_test[\"data\"])):\n",
        "  paragraphs = np.array([squad_test[\"data\"][doc][\"paragraphs\"][para][\"context\"] for para in range(len(squad_test[\"data\"][doc][\"paragraphs\"]))])\n",
        "  paragraph_series = pd.Series(paragraphs)\n",
        "  list_of_para_per_doc_test.append(paragraph_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poPH2u9_Wxnf",
        "colab_type": "text"
      },
      "source": [
        "Convert data into series (for resolved text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr6qKRouWwg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list_of_para_per_doc = list()\n",
        "# for doc in range(len(squad_train)):\n",
        "#   paragraphs = np.array([squad_train[doc][para] for para in range(len(squad_train[doc]))])\n",
        "#   paragraph_series = pd.Series(paragraphs)\n",
        "#   list_of_para_per_doc.append(paragraph_series)\n",
        "\n",
        "# list_of_para_per_doc_test = list()\n",
        "# for doc in range(len(squad_test)):\n",
        "#   paragraphs = np.array([squad_test[doc][para] for para in range(len(squad_test[doc]))])\n",
        "#   paragraph_series = pd.Series(paragraphs)\n",
        "#   list_of_para_per_doc_test.append(paragraph_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDU-GsnS9sfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del squad_train\n",
        "del squad_test\n",
        "gc.collect()\n",
        "del gc.garbage[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Mtf0GM35Qx",
        "colab_type": "text"
      },
      "source": [
        "Generate BERT embedding for every token of every sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdQmyzDVGEYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_embeddings = []\n",
        "temp = 0\n",
        "for doc_series in range(len(list_of_para_per_doc)):\n",
        "  print(temp)\n",
        "  embed = bert_embedding(list_of_para_per_doc[doc_series], filter_spec_tokens=False,)\n",
        "  doc_embeddings.append(embed)\n",
        "  print(len(doc_embeddings[doc_series][0][0]))\n",
        "  temp+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aCaLUvUMOyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = 0\n",
        "testing_embeddings = []\n",
        "for test_series in range(len(list_of_para_per_doc_test)):\n",
        "  print(temp)\n",
        "  embed = bert_embedding(list_of_para_per_doc_test[test_series], filter_spec_tokens=False,)\n",
        "  testing_embeddings.append(embed)\n",
        "  print(len(testing_embeddings[test_series - 2][0][0]))\n",
        "  temp+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A-GLbfY4Bed",
        "colab_type": "text"
      },
      "source": [
        "Select the entity embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5tg5jEWjSO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_entity(string_entity):\n",
        "  temporary = \"\"\n",
        "  if string_entity.isalnum():\n",
        "    return string_entity\n",
        "  if len(string_entity) > 0 and string_entity[0].isalnum() == False:\n",
        "    return string_entity[0]\n",
        "  for s in string_entity:\n",
        "    if s.isalnum():\n",
        "      temporary += s\n",
        "    else:\n",
        "      return temporary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyeroYq6G224",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities_embs = []\n",
        "for entity_num in range(len(entitiesf1.entity)):\n",
        "  doc_embs = entitiesf1.doc[entity_num]\n",
        "  paragraph = entitiesf1.para[entity_num]\n",
        "  if doc_embs < len(doc_embeddings):\n",
        "    try:\n",
        "      entity = entitiesf1.entity[entity_num]\n",
        "      new_entity = entity.lower().strip('\"')\n",
        "      if ' ' in new_entity:\n",
        "        new_entity = new_entity[:new_entity.index(' ')]\n",
        "      if new_entity not in doc_embeddings[doc_embs][paragraph][0]:\n",
        "        ne = \"[CLS] \" + new_entity + \" [SEP]\"\n",
        "        tokenized_text = tokenizer.tokenize(ne) \n",
        "        new_entity_temporary = tokenized_text[0]\n",
        "        if new_entity_temporary in doc_embeddings[doc_embs][paragraph][0]:\n",
        "          new_entity = new_entity_temporary\n",
        "        else:\n",
        "          new_entity = split_entity(new_entity)\n",
        "      entity_index = doc_embeddings[doc_embs][paragraph][0].index(new_entity)\n",
        "      entities_embs.append(doc_embeddings[doc_embs][paragraph][1][entity_index])\n",
        "    except ValueError:\n",
        "      print(doc_embeddings[doc_embs][paragraph][0])\n",
        "      print(entity)\n",
        "      print(new_entity)\n",
        "      print(new_entity_temporary)\n",
        "      print(doc_embs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfsCjOniIB0G",
        "colab_type": "text"
      },
      "source": [
        "Embedding for Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiF2vlCKH_Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_embs = []\n",
        "for test_entity_num in range(len(testingf1.entity)):\n",
        "  test_embs = testingf1.doc[test_entity_num]\n",
        "  paragraph = testingf1.para[test_entity_num]\n",
        "  if test_embs < len(testing_embeddings):\n",
        "    try:\n",
        "      entity = testingf1.entity[test_entity_num]\n",
        "      new_entity = entity.lower().strip('\"')\n",
        "      if ' ' in new_entity:\n",
        "        new_entity = new_entity[:new_entity.index(' ')]\n",
        "      if new_entity not in testing_embeddings[test_embs][paragraph][0]:\n",
        "        ne = \"[CLS] \" + new_entity + \" [SEP]\"\n",
        "        tokenized_text = tokenizer.tokenize(ne) \n",
        "        new_entity_temporary = tokenized_text[0]\n",
        "        if new_entity_temporary in testing_embeddings[test_embs][paragraph][0]:\n",
        "          new_entity = new_entity_temporary\n",
        "        else:\n",
        "          new_entity = split_entity(new_entity)\n",
        "      entity_index = testing_embeddings[test_embs][paragraph][0].index(new_entity)\n",
        "      testing_embs.append(testing_embeddings[test_embs][paragraph][1][entity_index])\n",
        "    except ValueError:\n",
        "      print(testing_embeddings[test_embs][paragraph][0])\n",
        "      print(testingf1.entity[test_entity_num])\n",
        "      print(new_entity)\n",
        "      print(test_embs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwB_SeBr91vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del doc_embeddings\n",
        "del testing_embeddings\n",
        "gc.collect()\n",
        "del gc.garbage[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuvvM2sTG79-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities_embs = np.array(entities_embs)\n",
        "entities_embs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKUN55jLd545",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_embs = np.array(testing_embs)\n",
        "testing_embs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVYuHD0z4fsv",
        "colab_type": "text"
      },
      "source": [
        "PCA projection to 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgIYuACzRB91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entities_pca = PCA(n_components=2).fit_transform(entities_embs)\n",
        "entities_pca.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFk1URs7fem",
        "colab_type": "text"
      },
      "source": [
        "Evaluate a k-NN (k-Nearest Neighbour Classifier) model with LOOCV (Leave One Out Cross-Validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51S5Bl6DRkSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loocv = model_selection.LeaveOneOut()\n",
        "model = KNeighborsClassifier(n_neighbors=8)\n",
        "results = model_selection.cross_val_score(model, entities_embs, entitiesf1.Type, cv=loocv)\n",
        "print(\"Accuracy: %.3f%% (STDev %.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23_8R0X5GFr-",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIbC_hzB5M-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainedModel = model.fit(entities_embs, entitiesf1.Type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "962Ba5NwHLko",
        "colab_type": "text"
      },
      "source": [
        "Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t7V7nCCHLFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrong = 0\n",
        "total = 0\n",
        "predictions = trainedModel.predict(testing_embs)\n",
        "for i in range(len(testingf1.Type)):\n",
        "  if predictions[i] != testingf1.Type[i]:\n",
        "    print(i)\n",
        "    wrong += 1\n",
        "  total += 1\n",
        "\n",
        "print(str(total - wrong) + \" Correct out of \" + str(total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}